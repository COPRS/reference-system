
### Compliance to integration procedure v1.10 applied
##    Kafka configuration
#       Set replication count 
app.*.spring.cloud.stream.kafka.binder.replicationFactor=3
app.*.spring.cloud.stream.kafka.streams.binder.replicationFactor=3
app.*.spring.cloud.stream.kafka.streams.binder.configuration.replication.factor=3

#       Set retention for all topic 

# app.*.spring.cloud.stream.kafka.bindings.input.consumer.topic.properties.retention.ms=2678400000
# app.*.spring.cloud.stream.kafka.bindings.input.consumer.s2-l0c-part2.pw-l0c.properties.retention.ms=2678400000
# app.*.spring.cloud.stream.kafka.bindings.input.consumer.s2-l0c-job.properties.retention.ms=2678400000


#       Set partition count for all topics
app.pw-l0c.spring.cloud.stream.kafka.binder.autoAddPartitions=true
app.pw-l0c.spring.cloud.stream.kafka.binder.minPartitionCount=6

#       Set Lag balancing
app.pw-l0c.spring.cloud.stream.kafka.bindings.input.producer.configuration.partitioner.class=esa.s1pdgs.cpoc.message.kafka.LagBasedPartitioner
app.pw-l0c.spring.cloud.stream.kafka.binder.producerProperties.lag-based-partitioner.delay-seconds=30
app.pw-l0c.spring.cloud.stream.kafka.binder.producerProperties.lag-based-partitioner.consumer-group=s2-l0c-ew
app.pw-l0c.spring.cloud.stream.kafka.binder.producerProperties.lag-based-partitioner.topics-with-priority.s2-l0c-part2.pw-l0c=1

#       Set Long Processing parameters
app.ew-l0c.spring.cloud.stream.kafka.bindings.input.consumer.configuration.max.poll.records=1
app.ew-l0c.spring.cloud.stream.kafka.bindings.input.consumer.configuration.max.poll.interval.ms=7200000

#       Set Filter parameters 
app.s2-l0c-filter.spring.cloud.stream.kafka.bindings.input.consumer.configuration.max.poll.records=50
app.s2-l0c-filter.spring.cloud.stream.kafka.bindings.input.consumer.configuration.max.poll.interval.ms=600000
app.s2-l0c-filter.spring.cloud.stream.kafka.bindings.input.consumer.enableDlq=false



#       Version
app.*.common.rsChainVersion=1.8.0
version.pw-l0c=1.8.0
version.ew-l0c=1.8.0
version.s2-l0c-filter=3.2.1
version.s2-l0c-router=3.2.1

######################## Deployer #################################################################

deployer.*.kubernetes.image-pull-policy=Always
deployer.*.kubernetes.namespace=processing

deployer.*.kubernetes.liveness-probe-delay=10
deployer.*.kubernetes.liveness-probe-path=/actuator/health/liveness
deployer.*.kubernetes.liveness-probe-period=60
deployer.*.kubernetes.liveness-probe-port=8080
deployer.*.kubernetes.liveness-probe-timeout=60
deployer.*.kubernetes.readiness-probe-delay=60
deployer.*.kubernetes.readiness-probe-path=/actuator/health/readiness
deployer.*.kubernetes.readiness-probe-period=60
deployer.*.kubernetes.readiness-probe-port=8080
deployer.*.kubernetes.readiness-probe-timeout=20
deployer.*.kubernetes.max-terminated-error-restarts=3

deployer.pw-l0c.kubernetes.requests.memory=500Mi
deployer.pw-l0c.kubernetes.limits.memory=4000Mi
deployer.pw-l0c.kubernetes.requests.cpu=50m
deployer.pw-l0c.kubernetes.limits.cpu=2000m

deployer.ew-l0c.kubernetes.requests.memory=2000Mi
deployer.ew-l0c.kubernetes.limits.memory=24000Mi
# We increase cpu resquest to avoid ahavind too much POD on same node that are using a lot of disk.
# A better approach would be to set disk quota (PVC or other solution).
deployer.ew-l0c.kubernetes.requests.cpu=6000m
deployer.ew-l0c.kubernetes.limits.cpu=6000m

deployer.s2-l0c-filter.kubernetes.requests.cpu=160m
deployer.s2-l0c-filter.kubernetes.limits.cpu=300m
deployer.s2-l0c-filter.kubernetes.requests.memory=512Mi
deployer.s2-l0c-filter.kubernetes.limits.memory=600Mi
 

deployer.s2-l0c-router.kubernetes.requests.cpu=160m
deployer.s2-l0c-router.kubernetes.limits.cpu=300m
deployer.s2-l0c-router.kubernetes.requests.memory=400Mi
deployer.s2-l0c-router.kubernetes.limits.memory=600Mi


######################## App specific #############################################################


### Catalog config
app.pw-l0c.catalog.url=http://rs-metadata-catalog-searchcontroller-svc.processing.svc.cluster.local:8080
app.pw-l0c.catalog.timeout=5

### Chain config
app.*.chain.name=s2-l0c
app.*.chain.version=1.8.0

### Mongo config
app.pw-l0c.mongo.authenticationDatabase=admin
app.pw-l0c.mongo.database=s2-l0c-pw
app.pw-l0c.mongo.port=27017
app.pw-l0c.mongo.host=mongodb-0.mongodb-headless.database.svc.cluster.local

### Obs properties
app.*.obs.endpoint=https://oss.eu-west-0.prod-cloud-ocb.orange-business.com
app.*.obs.region=eu-west-0
app.*.obs.maxConcurrency=50
app.*.obs.maxThroughput=10
app.*.obs.minimumPartSize=5
app.*.obs.maxRetries=3
app.*.obs.downloadTimeout=15
app.*.obs.uploadTimeout=15
app.*.obs.bucket.auxBucket=ops-rs-s2-aux
app.*.obs.bucket.sessionBucket=ops-rs-session-files
app.*.obs.bucket.sadBucket=ops-rs-s2-sadata
app.*.obs.bucket.hktmBucket=ops-rs-s2-hktm
app.*.obs.bucket.l0DSBucket=ops-rs-s2-l0-ds
app.*.obs.bucket.l0GRBucket=ops-rs-s2-l0-gr

### Cleanup config
app.*.cleanup.localEnabled=true
app.*.cleanup.sharedEnabled=true
app.*.cleanup.hours=144

### Kafka Config
app.*.spring.kafka.bootstrap-servers=kafka-cluster-kafka-bootstrap.infra.svc.cluster.local:9092
app.*.spring.cloud.stream.kafka.binder.brokers=kafka-cluster-kafka-bootstrap.infra.svc.cluster.local:9092
app.*.spring.cloud.stream.kafka.binder.auto-create-topics=true
app.*.spring.cloud.stream.kafka.bindings.input.consumer.enable-dlq=true
app.*.spring.cloud.stream.kafka.bindings.input.consumer.dlq-name=error-warning
app.*.spring.cloud.stream.kafka.bindings.input.consumer.poll-timeout=5
app.*.spring.cloud.stream.bindings.input.consumer.max-attempts=1
# line repeated here 
app.s2-l0c-filter.spring.cloud.stream.kafka.bindings.input.consumer.enableDlq=false

app.pw-l0c.spring.cloud.stream.bindings.input.group=s2-l0c-pw
app.ew-l0c.spring.cloud.stream.bindings.input.group=s2-l0c-ew

### Preparation worker
deployer.pw-l0c.kubernetes.secret-refs=s2-l0c-pw
app.pw-l0c.spring.profiles.active=prod
app.pw-l0c.ps2.sharedFolderRoot=/shared
app.pw-l0c.ps2.demFolderRoot=/dem
deployer.pw-l0c.kubernetes.volume-mounts=[ { name: shared, mountPath: '/shared' }, { name: dem, mountPath: '/dem' } ]
deployer.pw-l0c.kubernetes.volumes=[ { name: shared, persistentVolumeClaim: { claimName: 's2-l0-shared', storageClassName: 'ceph-fs' } }, { name: dem, persistentVolumeClaim: { claimName: 's2-dem', storageClassName: 'ceph-fs' } } ]
deployer.pw-l0c.kubernetes.podSecurityContext={runAsUser: 1000}

### Execution worker
deployer.ew-l0c.kubernetes.secret-refs=s2-l0c-ew
app.ew-l0c.spring.profiles.active=prod
deployer.ew-l0c.kubernetes.volume-mounts=[ { name: shared, mountPath: '/shared' }, { name: dem, mountPath: '/dem' } ]
deployer.ew-l0c.kubernetes.volumes=[ { name: shared, persistentVolumeClaim: { claimName: 's2-l0-shared', storageClassName: 'ceph-fs' } }, { name: dem, persistentVolumeClaim: { claimName: 's2-dem', storageClassName: 'ceph-fs' } } ]
deployer.ew-l0c.kubernetes.podSecurityContext={runAsUser: 1000}

deployer.*.kubernetes.affinity.nodeAffinity={ requiredDuringSchedulingIgnoredDuringExecution: { nodeSelectorTerms: [ { matchExpressions: [ { key: 'node-role.kubernetes.io/processing', operator: 'In', values: 'common'}]}]}}
deployer.*.kubernetes.tolerations=[{key: 'node-role.kubernetes.io/processing', effect: 'NoSchedule'}]

###Filter
app.s2-l0c-filter.spring.cloud.stream.bindings.input.group=s2-l0c-filter
app.s2-l0c-filter.expression=((payload.additionalFields == null) || (payload.additionalFields.f5 != 'true')) && (     (payload.missionId=='S2' and payload.productFamily=='S2_AUX')     )
app.s2-l0c-filter.spring.cloud.stream.kafka.bindings.input.consumer.enableDlq=false
app.s2-l0c-filter.spring.cloud.stream.kafka.bindings.input.consumer.configuration.max.poll.records=50
app.s2-l0c-filter.spring.cloud.stream.kafka.bindings.input.consumer.configuration.max.poll.interval.ms=600000


